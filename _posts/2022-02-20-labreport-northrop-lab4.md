---
title: Lab 4 Notebook Entry
tags: Lab-Notebook-Entry
article_header:
  type: cover
  image:
    src: /IMG_0119.jpg
---

## **Completing Lab 4: Exploratory Voyant**

### Exploration Questions

*Question 1: How does changing Stopwords option from "auto detect" to none change the word cloud?*
Instead of largest/most frequent words being "humanities" and "students," they are now "and, of, the, said." Changes indicate that the most frequent words in the corpus are articles, conjunctions, and prepositions. That these words are filtered out by Voyant's pre-selected stopwords indicates that they are the most common words in any modern English language corpus.

*Question 2: Why is relative frequency sometimes a more useful or accurate measure of a term’s importance than raw frequency?*
Relative frequency indicates where in a corpus clusters of repetition occur. If a term is used repeatedly in one or several contexts but not used widely throughout a corpus, this could indicate something interesting is happening in this one context. To be honest, I'm still not fully clear on how to verbally express what is happening with relative frequency. In the documentation it doesn't give further definition of what "relative" is relative to. Itself per 10 million words? Would another way to describe it be how concentrated uses of the word are within the corpus? I'm going off the example of MCAS as having the highest relative frequency but a low raw frequency. In the Jane Austen sample corpus used on the documentation page, "Mr" and "Mrs" are the top terms in both raw and relative frequency. Thinking of relative frequency together with the TermsBerry (discussed below) of "Distinct Terms" helps a little bit, since MCAS had the largest bubble of all distinct terms, meaning it was used only in one document across the corpus but often in that document. 

*Question 3: What is this document? And, more importantly, what have you learned about the term “mcas” in our corpus overall, and/or the utility or value of “significance” metrics like TF-IDF?*
The document that contains "MCAS" is the abbreviation of a school used after the name of each student and/or staff member who signed a petition. The acronymn is defined here. "In mid-October, students and faculty members of the Morrissey College of Arts and Sciences Honors Program were notified via email that the program would be terminated after the graduation of the Class of 2021." That this acronym supposedly has a high "significance" shows that these metrics are only *possible* indicators of the importance of terms in a corpus; the metrics cannot read our minds to know what we are looking for or interested in, nor can they say "oh I see what's happening here with this school name topping the charts."

*Question 4: Looking through this list of terms and their “Comparison” values, what observations can you make about terms that are more likely to occur in the humanities corpus vs. terms that are more likely to occur in the science corpus? How are these terms different?*
Following the frequency rank in the humanities corpus, the first term that's more likely to appear in science corpus is "said," which is interesting. Maybe humanists vary their attribution verbs more? The next term with a negative comparison value is "science," so that tracks. Next I sorted the results by the comparison column in ascending order to display the smallest values, the "most negative" values that are most likely to occur in the science corpus. "Science, research, researchers, scientists" are now the top four terms, followed by "health, cancer, data, climate, said, computer, brain, cells." These are unsurprising terms in that if I were asked to create a list of "science words" I would probably chose very similar terms to those that are more likely to appear in the science corpus than the humanities.

Sorting in descending order, "students, humanities, faculty, studies, coruses, course, history, major, college, education, arts, classes" are the top terms, which paint a picture of the humanities as closely associated with the university experience and organization, whereas the science terms tracked with situations beyond the purview of the university.

*Question 5: What tool(s) did you explore? What did this tool(s) help you to observe about this data and/or what did you learn about this data using this tool(s)? Alternatively, what did you hope to learn about this data using this tool and how (or why) did reality seem to fall short of that expectation?*
I played with the TermsBerry by adjusting the number of terms it showed, the scale of difference between the various berry nodules, and toggling back and forth between "Top Terms" and "Distinct Terms." The biggest berry in the distinct terms was our friend MCAS. Most of the other terms were names. Terms like "grading" or "flim," when selected, showed zero colocations with other distinct terms. "South" had one correlation with "African" and "Persian" had one correlation with "Arabic." In a corpus of news articles, it seems like names might always be the bulk of distinct terms, versus in a novel or set of novels where character names are repeated enough to cease to be distinct and the distinct terms might be more noun/verb/adjective descriptive language.

Back in the top terms view, "students said humanities faculty" dominate the TermsBerry in terms of nodule size (frequency) and number of colocations with the other terms. (I removed the commas between those four terms because I think one of the fun aspects of Voyant is that it yields some lovely found poems.) "Students" colocates with almost every other top term. The colocations tool does show how some words appear in tandem as phrases, such as "Stanford review" and "general requirement." Most of the common terms are nouns that relate to the experience of being a university student or faculty memmber such as "curriculum committee academic program studies education."

Next I played with the "Contexts" tool by typing a word from the corpus into the search bar and reading the different snippets of text that contain the word. It feels like overhearing a bit of conversation on the train; I again feel like there is something poetic in reading fragments of writing here in their new context. While I understand the goal of Voyant and other DH tools is to generate quantitative data, I feel like they also give an interesting way to feel the qualitative texture of a set of texts. 

I was unable to get anything to display in the "Correlations" view; I tried typing in words in the search box, refreshing the tool, and looking in the documentation for tool for hints. 

### Response

In doing this week's lab of exploring a corpus with Voyant, I was most struck by the poetry generated (or perhaps encountered is a better verb) in the process. In ["Other People's Data: Humanities Edition,"](https://culturalanalytics.org/article/11822) Sarah Allison suggest the method of "treat[ing] the topic model more like a new manuscript: an unexplored object that deserves attention in its own right." I think the approach of reading humanities data output, whether in the form of someone's "final" topic model or the first-glance view Voyant offers, as its own text, one that is adjacent to the component texts of the corpus but is something other than a sum of them. I think this is possible because the idea of analyzing texts *as* a corpus is to do something different with them than if you were reading them one by one. By compiling texts into a corpus and feeding them into somesort of DH tool, there is a sort of authorship-via-aggregate happening and through the curation of works-into-corpus, a new, readable thing is made. Allison says that "A topic model of selected criticism is something like an argument and something like an archive." The process of compilation always involves some sort of argument, and to read the topic model, graph, or Voyant trend chart is to read for the argument that went into the compilation process. Beyond that, I think it can also be like reading an archive, where something over here reminds you of something over there, and the pieces fit together to tell a different story than a single document tells. What I found was that, similar to the way that autocorrections can sometimes be hilariously poetic or accidentally insightful, the "robotic" or automatic-ness of Voyant gave its output a sort of poetry. The Word Cloud and TermsBerry remind me of magnetic poetry--scrambled words and word parts waiting to be assembled into some sort of thought if not a grammatically complete sentence. The "Contexts" pane with 100 lines where the fulcrum term is "literature" with 10 (or 20, user's choice!) terms on either side balance out the line of the poem. Something about metrics that favor repetition, placement, and outliers feel like they have an underlying poetry to them, since repetition, what a word is next to, and the odd jolt of an unexpected word are poetic strategies. 

I feel confident stretching Voyant into artsy territory because of the ["DSC #6: Voyant's Big Day"](https://datasittersclub.github.io/site/dsc6.html) concluding "Note to Reader" about the passing of Stéfan Sinclair. The Data Sitters write that "Tools are more than mechanistic implementation of algorithms, as Critical Code Studies would point out, but you can get to the same point from a different perspective if you knew Stéfan and you experience Voyant." The something more than mechanics seems to be that Voyant generates views, ways of looking, as much as it generates statistical data. I don't fully understand what the "sparkline" graphs that show up in the Trends > Document Terms pane indicate, but, knowing that Stéfan wanted Voyant to be fun, I can imagine them as pulse lines, Voyant's EKG of how each word beats through the corpus. I think the "Dear Reader" profile of Stéfan is a helpful reminder of the stakes of humanities data: they will always be comparatively low. Our numbers don't need to make sense or be perfect because we are not drafting plans for bridges that will collapse if our calculations are not exact. We are reading. And when we read we can find patterns but we can also find poetry and lolz moments when the program totally didn't get what we were looking for. All that is ok because the whole point of reading is that it is different than engineering, both in the outcome and the process.
