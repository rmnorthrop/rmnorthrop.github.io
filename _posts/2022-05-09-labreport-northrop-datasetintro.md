---
title: Dataset Critical Introduction
tags: Lab-Notebook-Entry
article_header:
  type: cover
  image:
    src: /bookshelf.jpeg
---

## **1970s Bibliography of American Literature: A Dataset**

### **Dataset Description:**

The [1970s Bibliography of American Literature Dataset]( https://docs.google.com/spreadsheets/d/1eVIzo053ksP9c1OU5n1UARndQ0m0luoW7D4e-IOV4Ls/edit?usp=sharing) aims to compile a list of books published in the United States during the 1970s with the goal of providing an overview of literature that first became available to the US reading public within a single decade. To better describe the scope and spirit of this dataset, I offer a definition of each of the terms in its title.
“Bibliographies” as published volumes are a waning—if not extinct—genre of literary production, but the principle behind them as a resource for learning about what was written, by whom, what was published, by whom, when, and where is alive, well, and extensively expanded in contemporary digital humanities datasets. However, many of today’s humanities datasets about publication history are created with the intent to perform statistical analysis about the who, what, when, and where of publication. My goal was to build a dataset that is less a repository of data gathered for the purpose of conducting statistical analysis and more akin to the printed, hard copy bibliographies of the past, something that a scholarly or non-scholarly reader could flip (or scroll) through either intentionally or casually to learn something about the bibliography’s topic, which, in this case, is a period and a place. In later sections I will further discuss “readability” as a guiding principle informing my construction process and principles of inclusion.

“1970s” is as literal as it sounds; the dataset includes works with publication dates between 1970 and 1979. (My reason for choosing this decade over any other is due to personal scholarly interest in the economic, financial, technological, and literary events of these years.) “American” is a bit trickier. I have retained the term in a somewhat anachronistic sense. Today, literary scholars are more careful in their use of “US” as an adjective to indicate literature of the United States and “American” to indicate literature of the Americas—North, South, Central, and Caribbean. However, because this bibliography is concerned with works published in the 1970s, at that time the term “American,” especially when coupled with "Literature,” was used to mean literature of the United States of America. One of the main component datasets of this bibliography is HathiTrust records, which uses the language of “1st American ed.” to indicate the first US printing of a book originally published outside of the United States. For these reasons, I have chosen to keep an appellation that I recognize might be viewed as problematic or inaccurate but that I think best represents the ways the works included in the bibliography describe themselves in their own historical terms.
I use the term “Literature” in its most capacious sense to mean all books of all genres. I understand literature to include works that might be categorized as fiction, nonfiction, poetry, drama, history, biography, textbook, or anthology. I could have very well used the term “books” and considered it for much of the process of compiling this dataset, but I settled on “Literature” for the aesthetic reason that I like the sound of a collective noun representing the contents of the bibliography as a body of, as something whole, rather than a compilation of discrete “books.” This idea that, taken together, a list of books becomes its own sort of book, reflects the history of the bibliography as a printed volume and the future of datasets as readable manuscripts.
Finally, the word “Dataset” is a standard term in the Digital Humanities field but warrants definition nonetheless for those less familiar with Digital Humanities terminology (as I myself was prior to undertaking this project). A dataset is one set of records, represented in this case by rows in a .csv file (formatted as a Google Sheet and functionally similar to an Excel Spreadsheet file). A dataset is differentiated from a database, which is a collection of various datasets organized in such a way that a single query can draw information (data) from multiple datasets. I mention this because, perhaps, in the future this 1970s bibliography could become part of a 20th-Century US Literature database or a database of global 1970s literature, for example.
To assemble this dataset I drew from three existing datasets: [HathiTrust Post45 Fiction]( (https://view.data.post45.org/index), [20th-Century American Bestsellers] (http://bestsellers.lib.virginia.edu/decade/1970), and a personal Excel file complied over the past two years. The documentation for how I constructed this dataset is detailed in a separate Appendix and the pieces of the construction process are available in a separate [1970s Bibliography Construction]( https://docs.google.com/spreadsheets/d/1TRBSxJR8NILbZYYmg_S-rb1R5DHh_mJ4lVIPpa1XfV0/edit?usp=sharing) file.

## *Affordances, Limits, Curation, and Context*

The process of arriving at the definitions introduced above also guided the curation process for building this dataset. I limited the bibliography to the geographic boundary of the United States in order to provide a(n incomplete) snapshot of literature that was available to the US reading public during a single historical moment. This national limit is not meant to reinforce the US’s history of exceptionalism but rather to show how literature and ideas first generated within the US interacted with literature that was first published outside of the US’s borders and then translated and/or reprinted domestically. 
My choice to draw geographic distinction based on publication place differs from other definitions of “American Literature” that might privilege the nationality of the author. (Graham Greene, for example, is a British author whose works were widely read in the US during the 1970s, but, based on his claim of a UK home base and the subject matter of his books, his works might not be included in a bibliography of American literature created with different principles of inclusion.) I include his and other works by non-US writers because the goal of this bibliography is to create a view of cohabitating ideas, or literature that was circulating in the US reading public’s hands and minds at the same time, considering that the ideas and stories contained within could have—and might have been—in conversation. The geographic limit also serves to keep the bibliography to a readable length. 
In practice, however, I found that the HathiTrust component dataset had so many internationally published volumes that to simply eliminate them all felt reductive. I chose to include these non-US 1970s publications as a separate tab within the Dataset Construction file. This dataset serves as a preliminary counterpoint to what was being published in English or translated into English beyond the US during the same period. While my original goal in creating this dataset was not for comparative literary purposes, by giving the bibliography a nationally bounded scope it could be conceivably be used for that purpose.
The goal of the temporal boundary is to capture literature that is entering the US reading public’s purview during the same period; therefore, the dataset does not include reprints of works originally published in the US in English at any previous point. But, again, it does include works that were originally published in another country and saw their works published in the US for the first time during the 1970s. This also included translated works. 
Also with the goal of keeping the content readable, I elected not to include serial publications such as magazines, journals, and zines. This choice was mostly practical, as including each issue of serial publications from a decade would greatly multiply the number of records. A dataset specifically of 1970s serial publications and periodicals might be a future component to contribute to a broader 1970s literary database.
What I hope to be one main affordance of this dataset is the view it offers of “cohabitating ideas,” or stories that were floating in the same intellectual, conversational, and social space at the same time. Part of what drew me to study 1970s US literature was realizing that Kurt Vonnegut’s Breakfast of Champions and Toni Morrison’s Sula were both published in the same year, 1973, and that James Baldwin’s last novel Just Above My Head was published in 1979, when Stephen King’s also published two novels. I wanted to read those novels as a reader in 1973 or 1979 might have, picking up one then the other from the local bookstore and thinking about them in the context of contemporaneous politics and economics. I hope this dataset affords readers the opportunity to inhabit a historical period in ways they might not have previously considered and to find conversations that might have happened (or at least could have happened) between writers who are not commonly read and written about together.  

## *Illuminations: Learning Along the Way*

In the process of constructing this dataset, it occurred to me that the process might be the product, that constructing a dataset in and of itself might be a new method for literary scholars to intentionally undertake. In this process-as-product orientation, the final dataset is incidental and not really the point, more of a shareable byproduct to accompany the publication of more traditional literary essays.
Perhaps a narrative way of framing it is that what I learned along the way I could not have learned through close reading of works by a single author or of comparative reading or a few works by a few authors. For example, the observation I made about the circulation of Graham Greene’s novels I would not have been able to make without taking this wide view of US literary publications during the 1970s. Similarly, I can see patterns in anthologizing science fiction and how sci-fi authors also played the role of editors in a way that seems unique to the genre and might be an interesting line of further inquiry only because I spent time reading the records in the dataset and saw patterns of names and contents emerging. If I were to then read a selection of those anthologies and write a literary critical essay about them, I would have to credit the dataset creation process as being the method by which I found out there was something interesting happening with sci-fi short story collections.
In the process of removing international titles from the HathiTrust list, I also noticed the sheer number of UK publications held by libraries and wondered if this is still common. Do university libraries still purchase copies of foreign-published fiction, or today is it more common for a publisher to have an international imprint where the same book is released simultaneously in multiple countries and a US university does not have to seek an internationally-published copy? Did the UK have more sway and relevance to US authors and readers than today? Or maybe less? Did public libraries carry as many internationally published titles as university libraries, or is the percentage of international English-language fiction (including translations) particular to the holdings of higher education libraries?
The humanities data concept that was most helpful in making curatorial choices aligned with the idea that the final dataset itself need not be perfect and that the process could be valuable in and of itself was that of being “against cleaning.”
The stance of being “against cleaning” comes from an [article of the same name]( https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/07154de9-4903-428e-9c61-7a92a6f22e51) by Katie Rawson and Trevor Muñoz in the 2019 Debates in the Digital Humanities series. They write that “the phrase ‘data cleaning’ is a stand-in for longer and more precise definitions of what people are doing in the initial stages of data-intensive research” and advocate for including a description that illuminates what happens in this obfuscated initial 80% of humanities data work and alternatives to processes of “cleaning” that challenge “cleaning”’s implication of some inherent or underlying correct order for that data’s constituent elements.
My attempt at description over obfuscation is to lay out the dataset’s criteria for inclusion explicitly here in the introduction and to include an appendix that shows how I constructed the dataset. So far I have used the verbs “construct” and “compile” to qualify the work involved in assembling the data in this set. This is intentional and meant to indicate that I am putting together data that was already collected by other people at earlier times for different reasons. In the appendix I use the verbs “format” and “merge” to describe how I transform the data from the ways it appears in the original component datasets to the way it appears in the 1970s bibliography. Again, these verbs are intended to be descriptive of the work I actually undertook—removing rows, reordering and renaming columns—and to avoid any unwanted implications that I am somehow restoring the data in the bibliography to any “correct” or “rightful” place.
In their article, Rawson and Muñoz give the example of their own work with the New York Public Library’s Curating Menus dataset. They classify their work as that of “indexing” and explain that the index they have created contains “labels” that function as “nodes” to “point” to other “values” and explain that the resulting “index is about ingredients, meal structures, and cooking techniques. Someone else could re-index the material in a different way.” The mention of other users who might “re-index” the same data is an interesting sidestep away from ideas of precise reproducibility. The goal in being descriptive about their process of data manipulation (the lengthy, labor-intensive stage often swept into the term “cleaning”) is less so that someone else can do exactly what they have done to reproduce their findings or results (a la hard and social sciences), but instead that someone else could repeat the same process with intentional changes at specific steps in order to answer a different question, say about pricing or the number of items on menus over time.

As I began to manipulate the data from the three constituent datasets, to change its content and appearance and then to combine these changed parts, I documented each step (and created “breadcrumb” tabs in the dataset construction .csv file) with the idea that someone else might want to follow my steps but at some point do something differently. Maybe someone else is interested in works by the same author and within the 1970s wants to format their dataset according to author clusters. Rather than quietly cleaning behind the scenes, I chose to vocally construct and format in a way that an outsider can follow along to understand what I did, in what order, and then potentially elect to make a different decision for a different reason (or for the same reason, if they disagree with my interpretation of “1970s” and want to include reprints or of “American” and want to include all of the Americas, for example).
Starting with the idea of constructing a dataset that I will be vocally organizing (rather than silently cleaning) led me to approach the construction process with more confidence. Rather than worrying about making the “wrong” decision about what to include or not include or what field name to use or not use, I simply made a choice, documented how and why I made that choice, and left the breadcrumbs there should someone else want to make a different choice. This eliminated the pressure of feeling like I had to get everything “right” and prompted me to think more deeply about my geographic, temporal, and generic decisions (time I might otherwise have spent in anxious circular arguments with myself). I find the concept of indexing (or breadcrumb-dropping) over cleaning to be productive because I know I can also change my own mind and go back and repeat the process, starting from any point, to make a different decision.
The second guiding principle for constructing this dataset was the idea of approaching a dataset as a manuscript, a manuscript which is itself an archive and an argument. In [“Other people’s data: Humanities edition,”]( https://culturalanalytics.org/article/11822-other-people-s-data-humanities-edition) Sarah Allison encourages scholars to build on other people’s data, to use the tools they have constructed (in order to create a graph or other visualization, in her example) as “a starting point for future research” (3). Taking up the popular digital humanities practice of topic modeling, Allison suggests that literary scholars should not hesitate to “treat the topic model more like a new manuscript: an unexplored object that deserves attention in its own right” (3). Allison’s suggestion approaches humanities data from the opposite side of everything I have discussed so far in my own suggestion that the **process** of working with humanities data might be more meaningful and illuminating than any final product created; her perspective begins with the product and treats it as that: a beginning, rather than an end.
This perspective, of recognizing that a topic model or other dataset read as its own manuscript can then become “something like an argument and something like an archive,” reminded me not to get so lost in the process of creating that I did not actually build anything. Again, instead of feeling that my 1970s bibliography had to be some authoritative and “right” documentation of that period of literary production in the US, I approached the project like a writing project, in which it is perfectly acceptable that the author’s fingerprints be visible on the final (albeit not finalized!) product. The same way that printed bibliographies vary based on the personalities and convictions of the scholars compiling them, this bibliographic dataset can look something like my own bookshelf might look, organized but with quirks particular to my own way of thinking.
Approaching the dataset-as-manuscript also led me to emphasize readability, which turned out to be a very helpful criteria when making curatorial choices about the inclusion and exclusion of both records and fields. If I want the final dataset to be readable, browsable, and skimmable in the way that a manuscript is, then I want to omit fields with predominately numeric data, such as codes or statistical calculations. If I want the data from the three constituent sets to be readable as a whole, then I do need to format field content with some consistency. Thinking of the dataset as a starting point is also congruent with the function of a bibliography; as a research tool, bibliographies are meant to point to books one might want to read, not to lead to statistical conclusions about an author, period, genre, or title. This is why the data in the 1970s bibliography is largely textual and includes blank fields, fields with “NA,” and other irregularities. The user I have in mind for this dataset is not a computer or a code but a human reader who might want to peruse the bibliography and notice things in the way I noticed things about sci-fi anthologies and Graham Greene’s fiction while compiling it.

## *Where It Stands*

As it stands, the 1970s bibliography is very much a work in progress. I detail its current stage further in the Appendix, but I will mention here that it includes only records from the HathiTrust dataset and not from 20th-Century Bestseller list or my personal dataset. I have yet to merge my list because both HathiTrust and 20thCB format author names as “Lastname, Firstname” and on my list I have “Firstname Lastname” and in the case of multiple authors or editors “Firstname Lastname and Firstname Lastname.” HathiTrust has only one name in the author field; in the case of multiple authors or editors, the author field contains “NA” and both names are included within the title field. Because my list includes author names that are three words long or include two names, I did not see a way to quickly reformat the author field of those 200+ records using Regular Expressions. I thought about merging the records with author names as they are, because the information is still readable, but ability to filter the list by author name would be disrupted, and I do want filter features to be functional so that users can view the dataset in the way that is most meaningful to them. ~200 records is a manageable number to reformat, so I left that as a manual task for the future. 
An additional manual task for the future is to check to see which of the 102 books on the 20th-C Bestseller list are already represented by records in the formatted portion of the HathiTrust dataset. Since the HathiTrust records have more data (publisher, place, contents) and since there is no quick way to check for title duplicates once the two component lists have been merged, it makes more sense to manually add the 20th-C Bestseller data to the “rank” field than to create a duplicate record that is comparatively empty. Since the same comparative emptiness is true of the limited fields in my personal list, I might also complete the same “ctrl+f” quick search to see if a record already exists in the HathiTrust component list before adding it (and I would do this prior to manually reformatting names in the author field, since records that will not be merged do not need to be formatted for merging). These kinds of order of operations considerations have been a consistent part of the puzzle of creating this dataset.

Because I chose to approach the project as a construction project comparable to writing manuscript, I think it is not a flaw that the merging of each component lists requires a substantial amount of manual formatting labor. As we discussed in class, people do more DH work than computers do. Separating non-US from US publications within the 8,876 records on the HathiTrust 1970s component list took between three and four hours. It required me to really read the dataset and the result was a HathiTrust 1970s component of 6,223 records and a new dataset of HathiTrust 1970s global publications with 2,453 records. Because data in the “publication place” field included cities, states/provinces, countries, and abbreviations, it was not a task a code or a computer could have completed. It required the application of my own geographical knowledge and using data in the “publisher” field, often a university press, to corroborate the country in the case of city names that repeat across nations.
Formatting, or not-cleaning, it more labor-intensive than it seems like it would be. Taking the 11,000+ records from the existing HathiTrust dataset with an “imprint date” of 1970-1979 and then removing records that do not meet the dataset’s principles of inclusion took roughly ten hours in total. However, I feel that labor is well-spent because the resulting beginning of the 1970s bibliography dataset reads as I envisioned it might, like traveling back in time to browse the bookstore or library shelves of the US 1970s to see what people are writing about and what people are reading. 

Works Cited

Allison, Sarah. 2016. “Other People’s Data: Humanities Edition.” Journal of Cultural Analytics 1 (1). https://doi.org/10.22148/001c.11822.
Muñoz, Trevor and Rawson, Katie. 2019. “Against Cleaning.” Debates in the Digital Humanities 2019. Regents of the University of Minnesota. https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/07154de9-4903-428e-9c61-7a92a6f22e51 

Photo: Selections from Rachel's 70s shelf
